{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generators\n",
    "\n",
    "### First-last harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_generate(n = 10, length = 10, grammatical = True, \n",
    "                        vowels = None, transparent = None):\n",
    "    \"\"\" Generates a collection words following the rule of first-last harmony.\n",
    "    \n",
    "    * n (int): number of strings that need to be generated;\n",
    "    * length (int): length of every one of the generated strings;\n",
    "    * grammatical (bool): if set to True, the correctly harmonizing\n",
    "                          forms are generated, and if set to False,\n",
    "                          the disharmonic forms are produced;\n",
    "    * vowels (list): list of vowels among which the first-last\n",
    "                     agreement is established;\n",
    "    * transparent (list): list of irrelevant elements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization and sanity check for the list of vowels\n",
    "    if vowels is None:\n",
    "        vowels = [\"a\", \"o\"]\n",
    "    elif len(vowels) < 2:\n",
    "        raise IndexError(\"The vowel system needs to contain at least two distinct vowels.\")\n",
    "    \n",
    "    # initialization and sanity check for the list of transparent elements\n",
    "    if transparent is None:\n",
    "        transparent = [\"x\"]\n",
    "    elif [i for i in vowels if i in transparent]:\n",
    "        raise ValueError(\"Lists of harmonizing vowels and transparent elements cannot overlap.\")\n",
    "    \n",
    "    # generate the required number of harmonic strings\n",
    "    strings = []\n",
    "    for i in range(n):\n",
    "        new = choice(vowels)\n",
    "        new += \"\".join([choice(vowels + transparent) for j in range(length - 2)])\n",
    "        if grammatical:\n",
    "            new += new[0]\n",
    "        else:\n",
    "            new += choice([i for i in vowels if i != new[0]])\n",
    "            \n",
    "        strings.append(new)\n",
    "\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oaaoxaxaxo', 'axoxaaoooa', 'oooaaoaaoo', 'oxooxxoxao', 'oaoaoaxxao']\n",
      "['axaooxxxoo', 'ooxoaxaxxa', 'aaooaxxooo', 'aoaaoooaxo', 'oaxoxxxxoa']\n"
     ]
    }
   ],
   "source": [
    "print(first_last_generate(n = 5, grammatical = True))\n",
    "print(first_last_generate(n = 5, grammatical = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc_harmony_generate(n = 10, length = 10, grammatical = True,\n",
    "                       vowels = None, consonants = None):\n",
    "    \"\"\" Generates a collection words following the rule of vowel-consonant harmony.\n",
    "    \n",
    "    * n (int): number of strings that need to be generated;\n",
    "    * length (int): length of every one of the generated strings;\n",
    "    * grammatical (bool): if set to True, the correctly harmonizing\n",
    "                          forms are generated, and if set to False,\n",
    "                          the disharmonic forms are produced;\n",
    "    * vowels (list): list of vowels among which the agreement\n",
    "                     is established;\n",
    "    * consonants (list): list of consonants among which the agreement\n",
    "                         is established.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization and sanity check for the list of vowels\n",
    "    if vowels is None:\n",
    "        vowels = [\"a\", \"o\"]\n",
    "    elif len(vowels) < 2:\n",
    "        raise IndexError(\"The vowel system needs to contain at least two distinct vowels.\")\n",
    "        \n",
    "    # initialization and sanity check for the list of consonants\n",
    "    if consonants is None:\n",
    "        consonants = [\"p\", \"b\"]\n",
    "    elif len(consonants) < 2:\n",
    "        raise IndexError(\"The consonant system needs to contain at least two distinct consonants.\")\n",
    "    elif [i for i in vowels if i in consonants] or [i for i in consonants if i in vowels]:\n",
    "        raise ValueError(\"Lists of harmonizing vowels and transparent elements cannot overlap.\")\n",
    "        \n",
    "    # generate the required number of harmonic strings\n",
    "    strings = []\n",
    "    for i in range(n):\n",
    "        v = choice(vowels)\n",
    "        c = choice(consonants)\n",
    "        new = \"\".join([choice([v, c]) for j in range(length)])\n",
    "        \n",
    "        # the ungrammatical forms are created by taking a random index\n",
    "        # and rewriting it to the opposite vowel / consonant\n",
    "        if not grammatical:\n",
    "            ind = choice(range(length))\n",
    "            if new[ind] == c:\n",
    "                new = new[:ind] + choice([i for i in consonants if i != c]) + new[ind + 1:]\n",
    "            else:\n",
    "                new = new[:ind] + choice([i for i in vowels if i != v]) + new[ind + 1:]\n",
    "                \n",
    "        strings.append(new)\n",
    "    \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbbbbabaa', 'aapaapaaap', 'aaababaabb', 'oooooobboo', 'aaabaaaabb']\n",
      "['bopooooobo', 'aapppaoaaa', 'poappooopo', 'obbbobbpbo', 'ooboooboab']\n"
     ]
    }
   ],
   "source": [
    "print(vc_harmony_generate(n = 5, grammatical = True))\n",
    "print(vc_harmony_generate(n = 5, grammatical = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure sensitive muti-tier harmony\n",
    "\n",
    "#### 1. Preparing a class to encode structure sensitive rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSRule(object):\n",
    "    \"\"\" A generic template for a structure-sensititve rule. \n",
    "    \n",
    "    * symbols (tuple): list of tier symbols relevant for the generalization;\n",
    "    * target (str): a target character context of which is important;\n",
    "    * right_context (str): a context in which a target character\n",
    "                           is projected on the tier;\n",
    "    * can_follow (tuple): a list of tier symbols that are allowed after\n",
    "                         the target character is projected.\n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, target, right_context, can_follow):\n",
    "        self.symbols = symbols\n",
    "        self.target = target\n",
    "        self.right_context = right_context\n",
    "        self.can_follow = can_follow\n",
    "\n",
    "    def is_grammatical(self, string):\n",
    "        \"\"\" Checks if the given form follows a rule that is encoded.\n",
    "        \n",
    "        * string (str): a string well-formedness of which needs to be checked.\n",
    "        \"\"\"\n",
    "        \n",
    "        # first, we get rid of all irrelevant symbols (not symbols and contexts)\n",
    "        string = \"\".join([i for i in string if i in list(self.symbols) + [self.right_context]])\n",
    "        \n",
    "        # second, we construct a tier of that strings\n",
    "        tier = \"\"\n",
    "        for i in range(len(string)):\n",
    "            if string[i] in self.symbols:\n",
    "                if string[i] == self.target and i < len(string) - 1 and\\\n",
    "                    string[i + 1] == self.right_context:\n",
    "                    tier += self.target\n",
    "                elif string[i] != self.target:\n",
    "                    tier += string[i]\n",
    "\n",
    "        # third, we check if that tier is well-formed\n",
    "        for t in range(len(tier)):\n",
    "            if tier[t] == self.target and t < len(tier) - 1 and\\\n",
    "                tier[t + 1] not in self.can_follow:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1 = SSRule(symbols = (\"o\", \"e\", \"a\"), target = \"o\", right_context = \"x\", can_follow = (\"a\", \"o\"))\n",
    "R1.is_grammatical(\"oxoeaee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True expected True\n",
      "True expected True\n",
      "True expected True\n",
      "False expected False\n"
     ]
    }
   ],
   "source": [
    "# prose: vowels \"o\", \"e\", and \"a\" are projected on the tier, but \"o\" can only be projected\n",
    "#        if it is immediately followed by \"x\". On the tier, after \"o\" we can only observe\n",
    "#        \"a\" or \"o\", i.e. projected \"o\" cannot be followed by \"e\" over the tier.\n",
    "R1 = SSRule(symbols = (\"o\", \"e\", \"a\"), target = \"o\", right_context = \"x\", can_follow = (\"a\", \"o\"))\n",
    "\n",
    "# the tier is \"aa\", \"o\" is not projected because it is not followed by \"x\"\n",
    "print(R1.is_grammatical(\"baboa\"), \"expected True\") \n",
    "# the tier is \"aoa\", \"o\" is projected and followed by \"a\" (allowed)\n",
    "print(R1.is_grammatical(\"baboxa\"), \"expected True\")\n",
    "# the tier is \"ae\", \"o\" is not projected because it is not followed by \"x\"\n",
    "print(R1.is_grammatical(\"baboe\"), \"expected True\")\n",
    "# the tier is \"aoe\", \"o\" is projected and followed by \"e\" (NOT allowed)\n",
    "print(R1.is_grammatical(\"baboxe\"), \"expected False\")\n",
    "\n",
    "\n",
    "# similarly, encoding another rule to make sure that we are dealing with multiple tiers\n",
    "R2 = SSRule(symbols = (\"b\", \"p\", \"d\"), target = \"b\", right_context = \"y\", can_follow = (\"b\", \"p\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Writing a generator of a sequence grammatical wrt the rule\n",
    "TODO: make this function a method in the SSRule class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rule_sequence(rule, length = 7, grammatical = True):\n",
    "    \"\"\" This function generates a sequence of symbols (un)grammatical \n",
    "        with respect to the given rule.\n",
    "        \n",
    "    * rule (SSRule): a rule describing a structure sensitive dependency;\n",
    "    * length (int): length of the generated sequence;\n",
    "    * grammatical (bool): produces correct form when set to True, and \n",
    "                          makes a mistake when set to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the generation of the well-formed sequence is done by a simplistic FSA\n",
    "    sequence = \"\"\n",
    "    state = 0\n",
    "    for i in range(length):\n",
    "        \n",
    "        # State 0: the target was not observed\n",
    "        if state == 0:\n",
    "            sequence += choice(list(rule.symbols) + [rule.right_context])\n",
    "            if sequence[-1] == rule.target:\n",
    "                state = 1\n",
    "                \n",
    "        # State 1: the target was observed\n",
    "        elif state == 1:\n",
    "            sequence += choice(list(rule.symbols) + [rule.right_context])\n",
    "            if sequence[-1] == rule.right_context:\n",
    "                state = 2\n",
    "            elif sequence[-1] != rule.target:\n",
    "                state = 0\n",
    "                \n",
    "        # State 2: the right context was observed\n",
    "        elif state == 2:\n",
    "            sequence += choice(list(rule.can_follow) + [rule.right_context])\n",
    "            if sequence[-1] in rule.can_follow and sequence[-1] != rule.target:\n",
    "                state = 0\n",
    "                \n",
    "    # if the ungrammatical form is needed, a violating sequence is generated\n",
    "    # and inserted into a random position within the sequence\n",
    "    if not grammatical:\n",
    "        violate = rule.target + rule.right_context +\\\n",
    "            choice([i for i in list(rule.symbols) if i not in rule.can_follow])\n",
    "        index_violate = choice(range(length - 3))\n",
    "        sequence = sequence[:index_violate] + violate + sequence[index_violate + 3:]\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correctness of the sequence generator is verified.\n"
     ]
    }
   ],
   "source": [
    "examples1 = [generate_rule_sequence(R1) for i in range(1000)]\n",
    "v1 = all(map(R1.is_grammatical, examples1))\n",
    "examples2 = [generate_rule_sequence(R2) for i in range(1000)]\n",
    "v2 = all(map(R2.is_grammatical, examples2))\n",
    "examples3 = [generate_rule_sequence(R1, grammatical = False) for i in range(1000)]\n",
    "v3 = not any(map(R1.is_grammatical, examples3))\n",
    "examples4 = [generate_rule_sequence(R2, grammatical = False) for i in range(1000)]\n",
    "v4 = not any(map(R2.is_grammatical, examples4))\n",
    "\n",
    "assert all([v1, v2, v3, v4]) == True\n",
    "print(\"The correctness of the sequence generator is verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intertwine\n",
    "Helper function for the next module, for taking two sequences that are grammatical wrt different rules and intertwining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intertwine(str1, str2, r = (0, 3)):\n",
    "    \"\"\" Intertwines two strings: str1 and str2. At every step, it takes\n",
    "    some characters from one string, and then some characters from another.\n",
    "    oxxooxa\n",
    "    * str1 (str): the first string;\n",
    "    * str2 (str): the second string;\n",
    "    * r (tuple[int, int]): min and max+1 symbols to be taken.\n",
    "    \"\"\"\n",
    "    new_string = \"\"\n",
    "    current = choice([1, 2])\n",
    "    while str1 or str2:\n",
    "        if current == 1:\n",
    "            cut = choice(range(r[0], r[1]))\n",
    "            if len(str1) < cut:\n",
    "                new = str1[:]\n",
    "            else:\n",
    "                new = str1[:cut]\n",
    "            new_string += new\n",
    "            str1 = str1[len(new):]\n",
    "            current = 2\n",
    "        elif current == 2:\n",
    "            cut = choice(range(r[0], r[1]))\n",
    "            if len(str2) < cut:\n",
    "                new = str2[:]\n",
    "            else:\n",
    "                new = str2[:cut]\n",
    "            new_string += new\n",
    "            str2 = str2[len(new):]\n",
    "            current = 1\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab1cd23e45f6g78'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intertwine(\"abcdefg\", \"12345678\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generator for the IMTSL harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imtsl_harmony_generate(n = 10, length = 10, grammatical = True,\n",
    "                       rule_1 = None, rule_2 = None):\n",
    "    \"\"\" Generates a collection words following the given rules of the structure-\n",
    "        Takessensitive dependencies that involve several tiers.\n",
    "    \n",
    "    * n (int): number of strings that need to be generated;\n",
    "    * length (int): length of every one of the generated strings;\n",
    "    * grammatical (bool): if set to True, the correctly harmonizing\n",
    "                          forms are generated, and if set to False,\n",
    "                          the disharmonic forms are produced;\n",
    "    * rule_1 (SSRule): the first rule describing a long-distant structure-\n",
    "                      sensitive dependency;\n",
    "    * rule_2 (SSRule): the second rule describing a long-distant structure-\n",
    "                      sensitive dependency.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the rules to R1 and R2 shown above\n",
    "    if rule_1 == None:\n",
    "        rule_1 = SSRule(symbols = (\"o\", \"e\", \"a\"), target = \"o\",\\\n",
    "                        right_context = \"x\", can_follow = (\"a\", \"o\"))\n",
    "    if rule_2 == None:\n",
    "        rule_2 = SSRule(symbols = (\"b\", \"p\", \"d\"), target = \"b\",\\\n",
    "                        right_context = \"y\", can_follow = (\"b\", \"p\"))\n",
    "    \n",
    "    strings = []\n",
    "    for i in range(n):\n",
    "        # generate two tiers independently, and then intertwine them\n",
    "        # WARNING: the tier alphabets of the two rules cannot overlap\n",
    "        #          (required by both learner and generator)\n",
    "        len_part_1 = length // 2\n",
    "        len_part_2 = length - len_part_1\n",
    "\n",
    "        part_1 = generate_rule_sequence(rule_1, len_part_1)\n",
    "        part_2 = generate_rule_sequence(rule_2, len_part_2)\n",
    "\n",
    "        if not grammatical:\n",
    "            mistake = choice([\"R1\", \"R2\", \"both\"])\n",
    "            if mistake == \"R1\":\n",
    "                part_1 = generate_rule_sequence(rule_1, len_part_1, grammatical = False)\n",
    "            elif mistake == \"R2\":\n",
    "                part_2 = generate_rule_sequence(rule_2, len_part_2, grammatical = False)\n",
    "            else:\n",
    "                part_1 = generate_rule_sequence(rule_1, len_part_1, grammatical = False)\n",
    "                part_2 = generate_rule_sequence(rule_2, len_part_2, grammatical = False)\n",
    "\n",
    "        # intertwining the two generated sequences\n",
    "        new_string = intertwine(part_1, part_2)\n",
    "        strings.append(new_string)\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yboapaxdobappyo', 'dxapybapxebeopb', 'apxeaayadpaybdy', 'xyppaoeedeadppd', 'apxabxxpexdpddd', 'bybexbobbexoypx', 'ppoxdbaoyoxpodd', 'bapyeaobdooybbo', 'daepyxddeaaoddy', 'ebpoexebaeyybbb', 'boayyooeebbabyb', 'doeyaoeyodpodbb', 'opybeapodpxobpa', 'ypybxapdpxxdxoa', 'yxdbebpaybopooe']\n"
     ]
    }
   ],
   "source": [
    "print(imtsl_harmony_generate(n = 15, length = 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correctness of the generator is verified.\n"
     ]
    }
   ],
   "source": [
    "good = imtsl_harmony_generate(n = 1000, length = 15)\n",
    "good_evals = list(map(all, zip(map(R1.is_grammatical, good), map(R2.is_grammatical, good))))\n",
    "bad = imtsl_harmony_generate(n = 1000, length = 15, grammatical = False)\n",
    "bad_evals = list(map(all, zip(map(R1.is_grammatical, bad), map(R2.is_grammatical, bad))))\n",
    "assert all(good_evals) == (not any(bad_evals)) == True\n",
    "print(\"The correctness of the generator is verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data samples ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(string):\n",
    "    return \">>\" + string + \"<<\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>aa<<', '>>aa<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>oo<<', '>>aoa<<', '>>axa<<', '>>axa<<', '>>aaa<<', '>>oxo<<', '>>ooo<<', '>>aaa<<', '>>axa<<', '>>axa<<', '>>aaa<<', '>>axa<<', '>>aoa<<', '>>oxo<<', '>>aaa<<', '>>ooo<<', '>>aaa<<', '>>ooo<<', '>>aoa<<', '>>axa<<', '>>oao<<', '>>aoa<<', '>>aaa<<', '>>aaa<<', '>>oxo<<', '>>ooo<<', '>>axa<<', '>>oxo<<', '>>oxo<<', '>>ooo<<', '>>aaa<<', '>>oxo<<', '>>oao<<', '>>aaa<<', '>>ooo<<', '>>ooo<<', '>>aoa<<', '>>aaa<<', '>>oao<<', '>>axa<<', '>>ooo<<', '>>oxo<<', '>>aoa<<', '>>aoa<<', '>>aaa<<', '>>oao<<', '>>aoa<<', '>>ooo<<', '>>ooo<<', '>>aaa<<', '>>ooo<<', '>>ooo<<', '>>aoa<<', '>>axa<<', '>>ooo<<', '>>aoa<<', '>>oxo<<', '>>aaa<<', '>>oxo<<', '>>aaa<<', '>>aaa<<', '>>aaa<<', '>>oao<<', '>>aoa<<', '>>ooo<<', '>>oxo<<', '>>aoa<<', '>>aoa<<', '>>aoa<<', '>>aoa<<', '>>aoa<<', '>>aaa<<', '>>axa<<', '>>oxo<<', '>>aaa<<', '>>ooo<<', '>>oao<<', '>>aaa<<', '>>aaa<<', '>>aoa<<', '>>ooo<<', '>>aaa<<', '>>oao<<', '>>oxo<<', '>>oxo<<', '>>oxo<<', '>>ooo<<', '>>oxo<<', '>>axa<<', '>>aaa<<', '>>oxo<<', '>>ooo<<', '>>axa<<', '>>aoa<<', '>>aoa<<', '>>oxo<<', '>>oao<<', '>>axa<<', '>>aaa<<', '>>aaa<<', '>>aoa<<', '>>aoa<<', '>>oao<<', '>>axa<<', '>>ooo<<', '>>oxo<<', '>>axa<<', '>>axa<<', '>>axa<<', '>>oao<<', '>>oxo<<', '>>ooo<<', '>>oxo<<', '>>aaa<<', '>>axa<<', '>>aoa<<', '>>ooo<<', '>>aoa<<', '>>ooo<<', '>>ooo<<', '>>oao<<', '>>axa<<', '>>aaa<<', '>>oxo<<', '>>oao<<', '>>aoa<<', '>>oao<<', '>>oxo<<', '>>axa<<', '>>aoa<<', '>>oao<<', '>>ooo<<', '>>oao<<', '>>aoa<<', '>>aaa<<', '>>axa<<', '>>axa<<', '>>oao<<', '>>oao<<', '>>ooo<<', '>>aaa<<', '>>oxo<<', '>>oxo<<', '>>axa<<', '>>oao<<', '>>ooo<<', '>>axa<<', '>>aaa<<', '>>ooo<<', '>>oao<<', '>>ooo<<', '>>aaa<<', '>>aoa<<', '>>oao<<', '>>aaa<<', '>>axa<<', '>>oxo<<', '>>oao<<', '>>aoa<<', '>>oao<<', '>>aoa<<', '>>aaa<<', '>>oao<<', '>>aoa<<', '>>oxo<<', '>>oao<<', '>>ooo<<', '>>oao<<', '>>oao<<', '>>aoa<<', '>>aoa<<', '>>oao<<', '>>aaa<<', '>>oao<<', '>>aaa<<', '>>aaa<<', '>>axa<<', '>>aoa<<', '>>oxo<<', '>>axa<<', '>>aaa<<', '>>aoa<<', '>>aoa<<', '>>oxo<<', '>>oao<<', '>>aaa<<', '>>ooo<<', '>>ooo<<', '>>oxo<<', '>>oao<<', '>>aaa<<', '>>oao<<', '>>oao<<', '>>aaa<<', '>>oxo<<', '>>aaa<<', '>>aoa<<', '>>ooo<<', '>>aaa<<', '>>aoa<<', '>>axa<<', '>>aaoa<<', '>>oxoo<<', '>>axoa<<', '>>oxoo<<', '>>aaoa<<', '>>oxxo<<', '>>aooa<<', '>>aoxa<<', '>>ooao<<', '>>oooo<<', '>>aaaa<<', '>>oaxo<<', '>>axoa<<', '>>oaoo<<', '>>oaao<<', '>>ooxo<<', '>>aooa<<', '>>aoaa<<', '>>oaxo<<', '>>oxoo<<', '>>aaoa<<', '>>axoa<<', '>>aoxa<<', '>>oaxo<<', '>>oaxo<<', '>>ooxo<<', '>>aaoa<<', '>>aaoa<<', '>>oxxo<<', '>>ooao<<', '>>oooo<<', '>>axaa<<', '>>aaoa<<', '>>oxoo<<', '>>axaa<<', '>>aaoa<<', '>>ooxo<<', '>>axxa<<', '>>axaa<<', '>>oxxo<<', '>>oxoo<<', '>>oooo<<', '>>aaaa<<', '>>aooa<<', '>>oaxo<<', '>>ooxo<<', '>>oxoo<<', '>>aooa<<', '>>ooxo<<', '>>aooa<<', '>>axoa<<', '>>axaa<<', '>>aooa<<', '>>oaxo<<', '>>axaa<<', '>>oaoo<<', '>>aaxa<<', '>>aoxa<<', '>>oxao<<', '>>axoa<<', '>>aaoa<<', '>>oxoo<<', '>>oaoo<<', '>>oxxo<<', '>>oaoo<<', '>>axoa<<', '>>aaaa<<', '>>oooo<<', '>>oooo<<', '>>aaoa<<', '>>axxa<<', '>>aoaa<<', '>>axaa<<', '>>aoaa<<', '>>aaxa<<', '>>oxao<<', '>>oooo<<', '>>oaxo<<', '>>oxoo<<', '>>ooxo<<', '>>oaoo<<', '>>oaao<<', '>>aaoa<<', '>>aoxa<<', '>>aaaa<<', '>>oxao<<', '>>oxao<<', '>>oaoo<<', '>>axaa<<', '>>ooxo<<', '>>oooo<<', '>>axaa<<', '>>aoaa<<', '>>axoa<<', '>>aaoa<<', '>>ooxo<<', '>>oooo<<', '>>oxao<<', '>>aoxa<<', '>>aaoa<<', '>>oxoo<<', '>>aooa<<', '>>ooao<<', '>>aaaa<<', '>>aaoa<<', '>>aoxa<<', '>>aoxa<<', '>>oaao<<', '>>aoaa<<', '>>aooa<<', '>>ooao<<', '>>oxxo<<', '>>axoa<<', '>>oxxo<<', '>>oxxo<<', '>>ooao<<', '>>aaxa<<', '>>aaxa<<', '>>oaao<<', '>>oxxo<<', '>>axxa<<', '>>oaao<<', '>>oxoo<<', '>>axxa<<', '>>oaxo<<', '>>oaxo<<', '>>aaxa<<', '>>oxoo<<', '>>aoxa<<', '>>ooxo<<', '>>aaaa<<', '>>ooxo<<', '>>aaoa<<', '>>aaxa<<', '>>aaaa<<', '>>oxxo<<', '>>aaxa<<', '>>aaoa<<', '>>oxxo<<', '>>axoa<<', '>>axaa<<', '>>oooo<<', '>>oaao<<', '>>axoa<<', '>>oxoo<<', '>>aoaa<<', '>>oxao<<', '>>oooo<<', '>>aaaa<<', '>>oxxo<<', '>>aooa<<', '>>aooa<<', '>>aoaa<<', '>>oxxo<<', '>>axxa<<', '>>aoaa<<', '>>oxao<<', '>>aaaa<<', '>>aaxa<<', '>>oaao<<', '>>oxoo<<', '>>aaoa<<', '>>oaoo<<', '>>oaxo<<', '>>oxxo<<', '>>ooao<<', '>>ooxo<<', '>>oaxo<<', '>>aooa<<', '>>aaxa<<', '>>axxa<<', '>>aaoa<<', '>>aaaa<<', '>>aaxa<<', '>>axaa<<', '>>oxao<<', '>>oxao<<', '>>aooa<<', '>>oaxo<<', '>>aoaa<<', '>>ooxo<<', '>>aoaa<<', '>>oaxo<<', '>>oooo<<', '>>ooao<<', '>>oxoo<<', '>>oaxo<<', '>>oxao<<', '>>ooxo<<', '>>aooa<<', '>>oaao<<', '>>oxao<<', '>>oxxo<<', '>>oxao<<', '>>oooo<<', '>>aaxa<<', '>>oaxo<<', '>>oaao<<', '>>oxao<<', '>>axaa<<', '>>oooxo<<', '>>ooooo<<', '>>oaaao<<', '>>aoaoa<<', '>>axoxa<<', '>>oxxao<<', '>>axooa<<', '>>axaaa<<', '>>axaxa<<', '>>ooooo<<', '>>oaoxo<<', '>>ooxxo<<', '>>ooaxo<<', '>>oaaxo<<', '>>axxxa<<', '>>oooao<<', '>>oxxao<<', '>>aoxaa<<', '>>axoxa<<', '>>oaxxo<<', '>>axaoa<<', '>>aoaxa<<', '>>oaxxo<<', '>>oxxao<<', '>>aaoaa<<', '>>oxooo<<', '>>oooxo<<', '>>oaoao<<', '>>oaoao<<', '>>oooxo<<', '>>oxaxo<<', '>>ooxao<<', '>>ooaoo<<', '>>oxaao<<', '>>axooa<<', '>>aaaxa<<', '>>axxoa<<', '>>aoaoa<<', '>>oaxoo<<', '>>oxaxo<<', '>>aooxa<<', '>>aaaaa<<', '>>oxxxo<<', '>>axxaa<<', '>>oaoxo<<', '>>aaxaa<<', '>>oaooo<<', '>>ooxoo<<', '>>aoxaa<<', '>>axxaa<<', '>>aoaaa<<', '>>oxaoo<<', '>>axoxa<<', '>>aoxaa<<', '>>axoaa<<', '>>aaoxa<<', '>>oxooo<<', '>>oxoxo<<', '>>aaoaa<<', '>>axaaa<<', '>>oxaao<<', '>>oxaxo<<', '>>ooaxo<<', '>>aoaoa<<', '>>aooxa<<', '>>oxoxo<<', '>>ooaao<<', '>>ooaoo<<', '>>aoaaa<<', '>>aoaaa<<', '>>oxxxo<<', '>>aooxa<<', '>>ooaao<<', '>>ooaxo<<', '>>aaooa<<', '>>aoaaa<<', '>>oaaoo<<', '>>oaxxo<<', '>>aaaaa<<', '>>axoaa<<', '>>oxxoo<<', '>>aaaxa<<', '>>ooaao<<', '>>oaaoo<<', '>>ooxao<<', '>>oxaao<<', '>>axoxa<<', '>>aaaaa<<', '>>oxaao<<', '>>aaxxa<<', '>>aaxoa<<', '>>aooxa<<', '>>oxaao<<', '>>oxaxo<<', '>>oaaoo<<', '>>oaooo<<', '>>aooaa<<', '>>oaxoo<<', '>>oxaoo<<', '>>oxoxo<<', '>>axxoa<<', '>>aoaxa<<', '>>ooxao<<', '>>aaxoa<<', '>>aoxoa<<', '>>oaxoo<<', '>>oaaoo<<', '>>ooaxo<<', '>>oxaoo<<', '>>oaxoo<<', '>>oxxao<<', '>>aoaxa<<', '>>oooxo<<', '>>aoxoa<<', '>>axooa<<', '>>oxaoo<<', '>>aaaxa<<', '>>oaaoo<<', '>>ooaxo<<', '>>aoxxa<<', '>>aaaaa<<', '>>aaaxa<<', '>>aooxa<<', '>>oxoxo<<', '>>aooaa<<', '>>oxxoo<<', '>>aoxxa<<', '>>oaooo<<', '>>ooxoo<<', '>>ooaxo<<', '>>aaaxa<<', '>>axxxa<<', '>>axxxa<<', '>>aoxxa<<', '>>aoaaa<<', '>>aaoxa<<', '>>ooaxo<<', '>>ooxxo<<', '>>oxoao<<', '>>axxaa<<', '>>aaxaa<<', '>>oxxao<<', '>>oxxoo<<', '>>oaxao<<', '>>oaoxo<<', '>>aoaxa<<', '>>ooaxo<<', '>>oaoxo<<', '>>aoxoa<<', '>>oooao<<', '>>axoaa<<', '>>oxaoo<<', '>>aaooa<<', '>>oxaoo<<', '>>oaaoo<<', '>>aoaaa<<', '>>axaxa<<', '>>aaaoa<<', '>>oxxao<<', '>>oxooo<<', '>>axaaa<<', '>>aoooa<<', '>>aaooa<<', '>>oaaoo<<', '>>oaxoo<<', '>>oaaoo<<', '>>axoxa<<', '>>oooxo<<', '>>aaooa<<', '>>oaxxo<<', '>>oxooo<<', '>>oooxo<<', '>>oaooo<<', '>>aoaoa<<', '>>axaoa<<', '>>oaaxo<<', '>>oooxo<<', '>>oxxao<<', '>>oxooo<<', '>>aoxxa<<', '>>aaooa<<', '>>oaoao<<', '>>aoaoa<<', '>>axoaa<<', '>>aaxoa<<', '>>aaaaa<<', '>>oxaao<<', '>>axxaa<<', '>>axoaa<<', '>>axxoa<<', '>>axaaa<<', '>>aoooa<<', '>>aoaoa<<', '>>axoaa<<', '>>oxooo<<', '>>oxaxo<<', '>>aoaxa<<', '>>aoaxa<<', '>>axxaa<<', '>>oaxxo<<', '>>>><<<<', '>>>>a<<<<', '>>>>o<<<<']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "FL_harmony = first_last_generate(n = 200, length = 2)\n",
    "FL_harmony.extend(first_last_generate(n = 200, length = 3))\n",
    "FL_harmony.extend(first_last_generate(n = 200, length = 4))\n",
    "FL_harmony.extend(first_last_generate(n = 200, length = 5))\n",
    "FL_harmony.extend([\">><<\", \">>a<<\", \">>o<<\"])\n",
    "FL_harmony = list(map(annotate, FL_harmony))\n",
    "#FL_harmony_bad = first_last_generate(n = 150, length = 5, grammatical = False)\n",
    "print(FL_harmony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTSL_harmony = vc_harmony_generate(n = 150, length = 7)\n",
    "#print(\"F\")MTSL_harmony_bad = vc_harmony_generate(n = 150, length = 7, grammatical = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMTSL_harmony = imtsl_harmony_generate(n = 250, length = 15)\n",
    "#IMTSL_harmony_bad = imtsl_harmony_generate(n = 250, length = 15, grammatical = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code of the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma1 = [\"a\", \"o\", \"x\"]\n",
    "sigma2 = [\"a\", \"o\", \"b\", \"p\"]\n",
    "sigma3 = [\"a\", \"o\", \"e\", \"x\", \"b\", \"p\", \"d\", \"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating only well-formed n-grams\n",
    "\n",
    "First, since we generate ngrams automatically, we need to have a checker that nothing like `a<<b`, `a><k`, or `fdh<` is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_formed_ngram(ngram):\n",
    "    \"\"\"Tells if the given ngram is well-formed. An ngram is ill-formed if:\n",
    "    * there is something in-between two start- or end-symbols\n",
    "      ('>a>'), or\n",
    "    * something is before start symbol or after the end symbol\n",
    "       ('a>'), or\n",
    "    * the ngram consists only of start- or end-symbols.\n",
    "    Otherwise it is well-formed.\n",
    "    Arguments:\n",
    "        ngram (str): The ngram that needs to be evaluated.\n",
    "    Returns:\n",
    "        bool: well-formedness of the ngram.\n",
    "    \"\"\"\n",
    "    start, end = [], []\n",
    "    for i in range(len(ngram)):\n",
    "        if ngram[i] == \">\":\n",
    "            start.append(i)\n",
    "        elif ngram[i] == \"<\":\n",
    "            end.append(i)\n",
    "\n",
    "    start_len, end_len = len(start), len(end)\n",
    "    if any([start_len == len(ngram), end_len == len(ngram)]):\n",
    "        return False\n",
    "    \n",
    "    if start_len > 0:\n",
    "        if ngram[0] != \">\":\n",
    "            return False\n",
    "        if start_len > 1:\n",
    "            for i in range(1, start_len):\n",
    "                if start[i] - start[i - 1] != 1:\n",
    "                    return False\n",
    "    \n",
    "    if end_len > 0:\n",
    "        if ngram[-1] != \"<\":\n",
    "            return False\n",
    "        if end_len > 1:\n",
    "            for i in range(1, end_len):\n",
    "                if end[i] - end[i - 1] != 1:\n",
    "                    return False\n",
    "                \n",
    "    # this part is different from the checker from the SigmaPie\n",
    "    # to avoid passing fourgrams such as \"><<<\"\n",
    "    if len(ngram) > 3 and (ngram[1] == \"<\" or ngram[2] == \">\"):\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all possible 4-grams based on the alphabet\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def generate_all_ngrams(alphabet, size):\n",
    "    all_of_them = [\"\".join(i) for i in product(alphabet + [\"<\", \">\"], repeat = size)]\n",
    "    return [i for i in all_of_them if well_formed_ngram(i)]\n",
    "\n",
    "sigma1 = [\"a\", \"o\", \"x\"]\n",
    "# print(generate_all_ngrams(sigma1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aax<', 'aox<', 'axx<', 'ax<<', 'oax<', 'oox<', 'oxx<', 'ox<<', 'xaax', 'xaox', 'xaxx', 'xax<', 'xoax', 'xoox', 'xoxx', 'xox<', 'xxax', 'xxox', 'xxxx', 'xxx<', 'xx<<', '>ao<', '>ax<', '>oa<', '>ox<', '>xaa', '>xao', '>xax', '>xa<', '>xoa', '>xoo', '>xox', '>xo<', '>xxa', '>xxo', '>xxx', '>xx<', '>x<<', '>>xa', '>>xo', '>>xx', '>>x<']\n"
     ]
    }
   ],
   "source": [
    "# see which 4-grams are not in data\n",
    "\n",
    "def unattested_ngrams(data, all_ngrams):\n",
    "    not_in_data = []\n",
    "    for f in all_ngrams:\n",
    "        found = False\n",
    "        for string in data:\n",
    "            if f in string:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            not_in_data.append(f)\n",
    "    return not_in_data\n",
    "\n",
    "unattested = unattested_ngrams(FL_harmony, generate_all_ngrams(sigma1, 4))\n",
    "print(unattested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('>a', ('ao', 'ox', 'xo', 'oa'), 'o<')}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all paths from those those 4-grams\n",
    "\n",
    "\n",
    "# NOTE: we cannot just look at all symbols when inserting/removing \n",
    "# from the set of possible tier alphabet items: to learn the whole class\n",
    "# of IMTSL grammars, we need to look at all pairs of symbols.\n",
    "# we check which other _pairs_ can be removed/added to every path X\n",
    "\n",
    "# paths (\"ab\", [X], \"cd\") : set of tuples\n",
    "\n",
    "import re\n",
    "paths = {}\n",
    "\n",
    "def string_paths(string, fourgram):\n",
    "    ind_first = [s.start() for s in re.finditer(fourgram[:2], string)]\n",
    "    ind_second = [s.start() for s in re.finditer(fourgram[2:], string)]\n",
    "    \n",
    "    if not (ind_first and ind_second):\n",
    "        return []\n",
    "    \n",
    "    paths = set()\n",
    "    \n",
    "    for f in ind_first:\n",
    "        for s in ind_second:\n",
    "            if f >= s:\n",
    "                continue\n",
    "                \n",
    "            middle = string[f+1:s+1]\n",
    "            in_between = tuple(set(middle[i:i+2] for i in range(len(middle) - 1)))\n",
    "            path = (fourgram[:2], in_between, fourgram[2:])\n",
    "            paths.add(path)\n",
    "\n",
    "    return paths\n",
    "\n",
    "# string_paths(\">>abcc<<\", \">ac<\")\n",
    "# string_paths(\">>oxao<<\", \"oxxa\")\n",
    "string_paths(\">>aoxoao<<\", \">ao<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the paths for the unattested ngrams\n",
    "\n",
    "# collect a list of paths for a particular (unattested) pair of bigrams\n",
    "def data_paths(data, fourgram):\n",
    "    total_paths = set()\n",
    "    for d in data:\n",
    "        total_paths = total_paths.union(string_paths(d, fourgram))\n",
    "        \n",
    "    return total_paths\n",
    "\n",
    "# print(data_paths(FL_harmony, \">ao<\"))\n",
    "\n",
    "def find_relevant_paths(data, unattested_ngrams):\n",
    "    relevant_paths = dict()\n",
    "    for un in unattested_ngrams:\n",
    "        relevant_paths[un] = data_paths(data, un)\n",
    "    return relevant_paths\n",
    "\n",
    "relevant_paths = find_relevant_paths(FL_harmony, unattested)\n",
    "# print(relevant_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>>oaoaaao<<', '>>ooaxxxo<<', '>>aoxxa<<', '>>axoxxoa<<', '>>oxaaoxo<<', '>>oaoaxxo<<', '>>aaxaoaa<<', '>>oaxoaoo<<', '>>oaaaaxo<<', '>>aaxxoaa<<', '>>aaoaxxa<<', '>>oxxoaoo<<', '>>aaaxaxa<<', '>>aaoaa<<', '>>oxaaxxo<<', '>>oaaoxo<<', '>>oaxooao<<', '>>oooaooo<<', '>>oxaaxo<<', '>>axxaaa<<', '>>oxxooo<<', '>>axxxa<<', '>>ooooxo<<', '>>aaoaooa<<', '>>aaaoaaa<<', '>>axaaaa<<', '>>ooaaao<<', '>>axoooxa<<', '>>aoaxooa<<', '>>aooxoa<<', '>>ooxxaoo<<', '>>axaxooa<<', '>>oaooxxo<<', '>>ooaxoao<<', '>>oxxaooo<<', '>>aoaxxxa<<', '>>oaooaxo<<', '>>aaaaxxa<<', '>>oaoxo<<', '>>oaxo<<', '>>aaooa<<', '>>axaooaa<<', '>>aoaoa<<', '>>oxaoao<<', '>>aoxoxxa<<', '>>axxaooa<<', '>>oxoaoao<<', '>>oooxxxo<<', '>>aaoaaaa<<', '>>axoa<<', '>>axooxoa<<', '>>aoxooaa<<', '>>oaaxooo<<', '>>oxxoxao<<', '>>aoaxaaa<<', '>>aooxaoa<<', '>>ooaaoo<<', '>>aooaoxa<<', '>>oaooxao<<', '>>oaooao<<', '>>oxaoxao<<', '>>axxoaoa<<', '>>ooaaxoo<<', '>>aoaaaa<<', '>>aaaooxa<<', '>>ooxaxoo<<', '>>ooxoooo<<', '>>aoxaxoa<<', '>>aaxaoa<<', '>>oxxooao<<', '>>oaaooao<<', '>>axxooaa<<', '>>oaxaxoo<<', '>>aoooaa<<', '>>oxxao<<', '>>aooxoxa<<', '>>ooxxoao<<', '>>aooxaxa<<', '>>oaoaxo<<', '>>oaaoooo<<', '>>oaaxoo<<', '>>oaaoxoo<<', '>>aaoooaa<<', '>>ooooaao<<', '>>aoaaaaa<<', '>>aaoaxa<<', '>>oaoooxo<<', '>>oxxaaao<<', '>>oxaxxoo<<', '>>axxxoa<<', '>>oxooxao<<', '>>aoaaa<<', '>>ooooooo<<', '>>aaaaooa<<', '>>oaxxoao<<', '>>axoaxa<<', '>>axaxa<<', '>>oxxoaao<<', '>>axxoaaa<<', '>>oooxooo<<', '>>aoxaxaa<<', '>>oxaooao<<', '>>aaaaxaa<<', '>>axaaxaa<<', '>>oaxaaxo<<', '>>ooxaoao<<', '>>axoaaa<<', '>>oxaaoo<<', '>>ooooaxo<<', '>>aooooxa<<', '>>oaaxaoo<<', '>>oxoaao<<', '>>axaaoaa<<', '>>oxxxaoo<<', '>>oaaooxo<<', '>>ooxoxao<<', '>>aooxaaa<<', '>>ooaxaao<<', '>>aoxxxa<<', '>>aaooxxa<<', '>>aooaaoa<<', '>>aoxxaxa<<', '>>aoooaoa<<', '>>ooaaaao<<', '>>axxxaaa<<', '>>ooaoaao<<', '>>oaoxooo<<', '>>oaxaaao<<', '>>axxa<<', '>>oxxxoao<<', '>>aaaxoxa<<', '>>oaoaao<<', '>>ooxxoxo<<', '>>aoaxoxa<<', '>>oaaaooo<<', '>>oaaxaao<<', '>>axxaaxa<<', '>>aaoaoxa<<', '>>aaooxoa<<', '>>aoxaaaa<<', '>>aaa<<', '>>aaooaa<<', '>>aoooxoa<<', '>>aoaoaa<<', '>>oxooo<<', '>>oxxxxao<<', '>>aaaooa<<', '>>oaxxxoo<<', '>>axxaxxa<<', '>>oxxaaxo<<', '>>aoaaooa<<', '>>axxooxa<<', '>>oxaoxxo<<', '>>oxxaxoo<<', '>>aaoaaa<<', '>>aoxoaxa<<', '>>axaaxxa<<', '>>oxoao<<', '>>oxoaaao<<', '>>oooxaoo<<', '>>oxxaoo<<', '>>oaoaoao<<', '>>aaoxa<<', '>>oxooaao<<', '>>oxaoaao<<', '>>oxaoxo<<', '>>aaaaaaa<<', '>>oxaxxao<<', '>>oxoxxo<<', '>>ooxooo<<', '>>oaxaoao<<', '>>oxxxxoo<<', '>>ooxao<<', '>>aaoxaoa<<', '>>aoxaooa<<', '>>>>a<<<<', '>>axaoaxa<<', '>>aoaxoaa<<', '>>axa<<', '>>ooxxoo<<', '>>oxxoxoo<<', '>>oooxo<<', '>>aaoaaxa<<', '>>axxaxa<<', '>>oaoaoxo<<', '>>aooxxoa<<', '>>ooxxao<<', '>>oxaxooo<<', '>>axoaoxa<<', '>>oxaoaxo<<', '>>oxoaoo<<', '>>aaaoaxa<<', '>>oxxxao<<', '>>axxxxoa<<', '>>oxaaao<<', '>>axaaaxa<<', '>>oxaxaxo<<', '>>ooo<<', '>>ooxo<<', '>>axxoaxa<<', '>>aaoaaoa<<', '>>oaoxao<<', '>>aoxxxxa<<', '>>oxaoooo<<', '>>axaoxaa<<', '>>oxoooao<<', '>>oooaxo<<', '>>oxxoooo<<', '>>oxoaoxo<<', '>>aoaxxoa<<', '>>ooaxxoo<<', '>>aooaoaa<<', '>>axxxxaa<<', '>>oaoaooo<<', '>>aaxoaa<<', '>>aoaaxxa<<', '>>oooaao<<', '>>aoaaaoa<<', '>>oaaoaoo<<', '>>axoaoa<<', '>>oxoxoxo<<', '>>axaxaxa<<', '>>ooaxaxo<<', '>>aaaxxxa<<', '>>oaxaxo<<', '>>oaoxoxo<<', '>>oxxoao<<', '>>axaaoxa<<', '>>aoxoxa<<', '>>aoooa<<', '>>oxoxoao<<', '>>axaoaaa<<', '>>axxoooa<<', '>>aoaxxa<<', '>>axxxxa<<', '>>aaaaoa<<', '>>oooaoao<<', '>>aoxxoa<<', '>>oaoxaao<<', '>>oao<<', '>>oxoxo<<', '>>oxaxoxo<<', '>>axaoxa<<', '>>oaxxaxo<<', '>>axoxa<<', '>>aoxaa<<', '>>oooxaao<<', '>>oo<<', '>>oooaxao<<', '>>axxoxa<<', '>>oxxaoxo<<', '>>axooxxa<<', '>>aaxxaoa<<', '>>aaxxaa<<', '>>aoxaaxa<<', '>>oxaaoao<<', '>>axoaa<<', '>>oaxooxo<<', '>>oxooxxo<<', '>>oaoaoo<<', '>>ooaooo<<', '>>aooaoa<<', '>>oooxoao<<', '>>axaaxoa<<', '>>oxoxooo<<', '>>oxxxoo<<', '>>aoxxoxa<<', '>>oxooaxo<<', '>>axooaa<<', '>>oxoaaxo<<', '>>aooaxxa<<', '>>>>o<<<<', '>>axaoxxa<<', '>>oxoaxo<<', '>>aoxa<<', '>>aoxaoa<<', '>>axaoaa<<', '>>axoaxoa<<', '>>oaxaxxo<<', '>>aooxxaa<<', '>>axxxaoa<<', '>>ooooao<<', '>>aooaooa<<', '>>aoxoxoa<<', '>>oxaoaoo<<', '>>oaoaxoo<<', '>>aaaoaa<<', '>>aoxxxoa<<', '>>oaxoxao<<', '>>axaa<<', '>>ooxxxo<<', '>>axoxaaa<<', '>>oxxoxxo<<', '>>oxaxxxo<<', '>>aoaaxa<<', '>>oxxo<<', '>>aaoa<<', '>>oaaaao<<', '>>oaoxoo<<', '>>aoxxoaa<<', '>>ooxaxao<<', '>>oaxaao<<', '>>oaaoxxo<<', '>>oaaoao<<', '>>axaoxoa<<', '>>aaaxa<<', '>>axaoaoa<<', '>>aoaoooa<<', '>>oaaaxao<<', '>>aaaxxa<<', '>>aaoaxaa<<', '>>oaaoaxo<<', '>>oaxao<<', '>>axoaaoa<<', '>>aoaxaxa<<', '>>aaxxxa<<', '>>oxaaxoo<<', '>>oaaxao<<', '>>oxxaxao<<', '>>aooxa<<', '>>ooaxao<<', '>>ooxaoo<<', '>>aaoxoaa<<', '>>oaooaoo<<', '>>aaaoxaa<<', '>>aoooxxa<<', '>>ooaoaxo<<', '>>aoaaxoa<<', '>>aaxoxa<<', '>>oxoxaao<<', '>>ooxoao<<', '>>oaaoo<<', '>>oxxxxo<<', '>>aooaaa<<', '>>ooao<<', '>>axaxaa<<', '>>oooxaxo<<', '>>oaaaxxo<<', '>>oooxoo<<', '>>aaaxaaa<<', '>>oooaxoo<<', '>>ooxaao<<', '>>aaaaxoa<<', '>>oaooooo<<', '>>oaao<<', '>>oooaaao<<', '>>oxxoo<<', '>>aaoxoxa<<', '>>axxoxoa<<', '>>oxooxo<<', '>>oxoxaxo<<', '>>aoxoaoa<<', '>>axxaoaa<<', '>>aooxaa<<', '>>ooaooao<<', '>>aoa<<', '>>ooaoxo<<', '>>aaooxaa<<', '>>oooxoxo<<', '>>aaxxaxa<<', '>>aaaoooa<<', '>>aaxxoxa<<', '>>aoaoxa<<', '>>oaooo<<', '>>oxaaaao<<', '>>oaxxo<<', '>>oxooaoo<<', '>>aoaxoa<<', '>>oaoxaoo<<', '>>ooxxaxo<<', '>>ooxxxoo<<', '>>oxaoo<<', '>>aoxaxxa<<', '>>oaaaaao<<', '>>oooooxo<<', '>>aaxoaoa<<', '>>ooxoo<<', '>>oaaoxao<<', '>>axaxxa<<', '>>oxxaao<<', '>>oxoxao<<', '>>ooaaxao<<', '>>aaaooaa<<', '>>ooxooao<<', '>>ooaao<<', '>>aaaaaa<<', '>>aooaaaa<<', '>>ooaoxoo<<', '>>ooaxxo<<', '>>oxaooo<<', '>>oaaxoxo<<', '>>oxooao<<', '>>aaaxaoa<<', '>>ooxoaoo<<', '>>ooaaaxo<<', '>>oxxaaoo<<', '>>oaxaoxo<<', '>>aoxaoaa<<', '>>axxxooa<<', '>>aaxooaa<<', '>>ooaoao<<', '>>axxxoaa<<', '>>ooaooxo<<', '>>aaoxooa<<', '>>aaxaaxa<<', '>>oxxoaxo<<', '>>ooaaoao<<', '>>oaxxaao<<', '>>oxaxoao<<', '>>axxxaa<<', '>>oooaaxo<<', '>>oxxooxo<<', '>>ooxaaao<<', '>>axaoooa<<', '>>ooxoaxo<<', '>>aaooxa<<', '>>axooooa<<', '>>aaooaxa<<', '>>axooaxa<<', '>>aaoaxoa<<', '>>oaxaooo<<', '>>aoxoa<<', '>>aaoaoa<<', '>>aoxaaa<<', '>>aoaooaa<<', '>>ooooxoo<<', '>>ooaaoxo<<', '>>oxaxaoo<<', '>>oooxxao<<', '>>oaxoaxo<<', '>>oaaaxo<<', '>>ooxxo<<', '>>aooooa<<', '>>oxoaxao<<', '>>>><<<<', '>>aoxxxaa<<', '>>oxxaxxo<<', '>>aoxxooa<<', '>>ooaoxxo<<', '>>axoaooa<<', '>>oxxaxo<<', '>>oooxxo<<', '>>oaooaao<<', '>>ooooo<<', '>>oaaxaxo<<', '>>axxxaxa<<', '>>aaaaaoa<<', '>>axaxxoa<<', '>>oxo<<', '>>axoxooa<<', '>>aaxooxa<<', '>>oaoxxo<<', '>>axaaxa<<', '>>oxaxxo<<', '>>axoooa<<', '>>aoaoaoa<<', '>>aaxxxaa<<', '>>ooxxooo<<', '>>aaaaxa<<', '>>ooxoxo<<', '>>axxaxaa<<', '>>axoxoaa<<', '>>oooaoxo<<', '>>oaxxooo<<', '>>oaxxoxo<<', '>>aaoaoaa<<', '>>aaaaoaa<<', '>>axooxa<<', '>>oaxoxxo<<', '>>aooaa<<', '>>aoooxaa<<', '>>oaaoaao<<', '>>aaxoaaa<<', '>>aooxoaa<<', '>>oaoaaoo<<', '>>oxaxo<<', '>>ooxxxxo<<', '>>aaoxoa<<', '>>aaxoooa<<', '>>oaoxaxo<<', '>>oxaxaao<<', '>>oaoxxxo<<', '>>aaxxoa<<', '>>ooaoo<<', '>>aaxxa<<', '>>aoaooxa<<', '>>aoxxaoa<<', '>>aaooaoa<<', '>>axaxoaa<<', '>>aooa<<', '>>ooxaxxo<<', '>>axxaoa<<', '>>aaxaxa<<', '>>axooaaa<<', '>>axaaa<<', '>>aoxoaaa<<', '>>axoxxa<<', '>>aaoxaxa<<', '>>axxoa<<', '>>oxxxoxo<<', '>>aaxaxxa<<', '>>oooooao<<', '>>oxoxoo<<', '>>aoxooxa<<', '>>aaoxxxa<<', '>>aoooaxa<<', '>>oxxxo<<', '>>axooxaa<<', '>>aaxoaxa<<', '>>oxoxxoo<<', '>>axaaaaa<<', '>>aaxa<<', '>>aoxxaa<<', '>>oxooooo<<', '>>aaxxxoa<<', '>>ooaoooo<<', '>>ooxaaxo<<', '>>aaoxxoa<<', '>>ooxooxo<<', '>>oaaxoao<<', '>>oxao<<', '>>oaaxxao<<', '>>oooaoo<<', '>>axaxoa<<', '>>aoxoooa<<', '>>aoooooa<<', '>>aaaoxxa<<', '>>aaxaxaa<<', '>>oaoxxoo<<', '>>aaoxaaa<<', '>>oxoaooo<<', '>>aaaxooa<<', '>>aaaaoxa<<', '>>axaoa<<', '>>aoaooa<<', '>>aooxxxa<<', '>>oaxoooo<<', '>>oooaxxo<<', '>>oxaaaoo<<', '>>aoaa<<', '>>oaoaxao<<', '>>oaxxao<<', '>>aaxaa<<', '>>aaaa<<', '>>aaaoa<<', '>>aaaaa<<', '>>oaxoxoo<<', '>>oxxxooo<<', '>>axoxaxa<<', '>>axxaxoa<<', '>>axoxoa<<', '>>oooaaoo<<', '>>ooaxo<<', '>>aaxoa<<', '>>ooaxooo<<', '>>oxoxaoo<<', '>>aoaxaa<<', '>>oaaaoxo<<', '>>aoaoxaa<<', '>>axxaa<<', '>>aoaaoaa<<', '>>axaaaoa<<', '>>oxaxao<<', '>>axoooaa<<', '>>axxxxxa<<', '>>oxaao<<', '>>oaaaoo<<', '>>aaoxxa<<', '>>ooxoaao<<', '>>oaooxo<<', '>>aoaaoa<<', '>>oaaxxxo<<', '>>aaxaxoa<<', '>>oooo<<', '>>aooxxa<<', '>>oaooxoo<<', '>>axoaaaa<<', '>>aaoxaa<<', '>>oaaaaoo<<', '>>oxaaooo<<', '>>oxoooxo<<', '>>oaoooao<<', '>>oaaxxo<<', '>>ooaxoxo<<', '>>axaooa<<', '>>oaxaxao<<', '>>oxaooxo<<', '>>aaxoxoa<<', '>>oaaaoao<<', '>>aoaxaoa<<', '>>ooxaaoo<<', '>>ooxaoxo<<', '>>aaooaaa<<', '>>oxoaaoo<<', '>>ooaaaoo<<', '>>aaaoaoa<<', '>>oxoxxxo<<', '>>aoxaxa<<', '>>axoaoaa<<', '>>oxoaxxo<<', '>>axaaoa<<', '>>aa<<', '>>oaxxxo<<', '>>oxxoxo<<', '>>aaooooa<<', '>>ooaxxao<<', '>>oaxoo<<', '>>aaaxaa<<', '>>axoxaa<<', '>>oaoooo<<', '>>axoaxxa<<', '>>oxoaxoo<<', '>>oaoo<<', '>>ooxaooo<<', '>>oxxxaxo<<', '>>aooooaa<<', '>>aoaxxaa<<', '>>oxaaaxo<<', '>>oxooxoo<<', '>>oxoo<<', '>>ooaaooo<<', '>>oaaao<<', '>>aaaxxaa<<', '>>oaxoao<<', '>>ooaoxao<<', '>>aaxaaa<<', '>>oooao<<', '>>oxxaoao<<', '>>aaxaaoa<<', '>>aoxoxaa<<', '>>oxoxxao<<', '>>axoaxaa<<', '>>ooxxaao<<', '>>oaxooo<<', '>>oooooo<<', '>>aaaaaxa<<', '>>aaxooa<<', '>>oaxaaoo<<', '>>axxoxxa<<', '>>aaaoxoa<<', '>>aaaoxa<<', '>>aaaxoaa<<', '>>aaxxxxa<<', '>>aaaxoa<<', '>>axxoaa<<', '>>ooooxao<<', '>>axaxaaa<<', '>>ooaxoo<<', '>>aaxoxaa<<', '>>ooxoxoo<<', '>>aaxxaaa<<', '>>aaxaooa<<', '>>axaooxa<<', '>>oaxoxo<<', '>>axooa<<', '>>oxaoxoo<<', '>>aoaaxaa<<', '>>axooaoa<<', '>>oxxxaao<<', '>>aooaxaa<<', '>>oaaxxoo<<', '>>ooooaoo<<', '>>axoxaoa<<', '>>oxxxxxo<<', '>>ooxoxxo<<', '>>aoaaaxa<<', '>>aaaxxoa<<', '>>oooxxoo<<', '>>ooxaxo<<', '>>aooaxa<<', '>>axxaaaa<<', '>>oaaooo<<', '>>ooaoaoo<<', '>>oaaaxoo<<', '>>oxaxoo<<', '>>axaxxxa<<', '>>aaoooxa<<', '>>ooxxxao<<', '>>oaoao<<', '>>aoxooa<<', '>>aaxxooa<<', '>>axaxoxa<<', '>>axxooa<<', '>>aoaaoxa<<', '>>ooooxxo<<', '>>ooaxaoo<<', '>>oaxxoo<<', '>>axaxxaa<<', '>>oaxoaao<<', '>>oaxxxxo<<', '>>aoooxa<<', '>>oxaaxao<<', '>>oooxao<<', '>>axaaooa<<', '>>aoxaaoa<<', '>>oaxxxao<<', '>>aoooaaa<<', '>>aaxaaaa<<', '>>aoxxaaa<<', '>>oaxxaoo<<', '>>aoxaoxa<<', '>>aaoxxaa<<', '>>aoaoxxa<<', '>>axoxxxa<<', '>>aoaoaaa<<', '>>axoaaxa<<', '>>oaaxo<<', '>>ooaaxo<<', '>>aoaxa<<', '>>aoaoaxa<<', '>>axxoxaa<<', '>>ooaaxxo<<', '>>axxxoxa<<', '>>oxoooo<<', '>>oaoxxao<<', '>>aooxooa<<', '>>axaxaoa<<', '>>axxaaoa<<', '>>oaoxoao<<', '>>aoxoaa<<', '>>aaoooa<<', '>>aaxoxxa<<', '>>aooaxoa<<', '>>aaxaoxa<<', '>>oaxaoo<<']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "FL_harmony = first_last_generate(n = 2000, length = 2)\n",
    "FL_harmony.extend(first_last_generate(n = 2000, length = 3))\n",
    "FL_harmony.extend(first_last_generate(n = 2000, length = 4))\n",
    "FL_harmony.extend(first_last_generate(n = 2000, length = 5))\n",
    "FL_harmony.extend(first_last_generate(n = 2000, length = 6))\n",
    "FL_harmony.extend(first_last_generate(n = 2000, length = 7))\n",
    "FL_harmony.extend([\">><<\", \">>a<<\", \">>o<<\"])\n",
    "FL_harmony = list(set(list(map(annotate, FL_harmony))))\n",
    "#FL_harmony_bad = first_last_generate(n = 150, length = 5, grammatical = False)\n",
    "print(FL_harmony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('>>', 'x<'): ['>>', 'x<'],\n",
      " ('>>', 'xa'): ['>>', 'xa', 'ao', 'ax', 'oa', 'ox', 'xo', '>a', '>o'],\n",
      " ('>>', 'xo'): ['>>', 'xo', 'ao', 'ax', 'oa', 'ox', 'xa', '>a', '>o'],\n",
      " ('>>', 'xx'): ['>>', 'xx', 'ao', 'ax', 'oa', 'ox', 'xa', 'xo', '>a', '>o'],\n",
      " ('>a', 'o<'): ['>a', 'o<'],\n",
      " ('>a', 'x<'): ['>a', 'x<'],\n",
      " ('>o', 'a<'): ['>o', 'a<'],\n",
      " ('>o', 'x<'): ['>o', 'x<'],\n",
      " ('>x', '<<'): ['>x', '<<'],\n",
      " ('>x', 'a<'): ['>x', 'a<'],\n",
      " ('>x', 'aa'): ['>x', 'aa'],\n",
      " ('>x', 'ao'): ['>x', 'ao'],\n",
      " ('>x', 'ax'): ['>x', 'ax'],\n",
      " ('>x', 'o<'): ['>x', 'o<'],\n",
      " ('>x', 'oa'): ['>x', 'oa'],\n",
      " ('>x', 'oo'): ['>x', 'oo'],\n",
      " ('>x', 'ox'): ['>x', 'ox'],\n",
      " ('>x', 'x<'): ['>x', 'x<'],\n",
      " ('>x', 'xa'): ['>x', 'xa'],\n",
      " ('>x', 'xo'): ['>x', 'xo'],\n",
      " ('>x', 'xx'): ['>x', 'xx'],\n",
      " ('aa', 'x<'): ['aa', 'x<'],\n",
      " ('ao', 'x<'): ['ao', 'x<'],\n",
      " ('ax', '<<'): ['ax', '<<', 'ao', 'a<', 'oa', 'ox', 'o<', 'xa', 'xo'],\n",
      " ('ax', 'x<'): ['ax', 'x<'],\n",
      " ('oa', 'x<'): ['oa', 'x<'],\n",
      " ('oo', 'x<'): ['oo', 'x<'],\n",
      " ('ox', '<<'): ['ox', '<<', 'ao', 'ax', 'a<', 'oa', 'o<', 'xa', 'xo'],\n",
      " ('ox', 'x<'): ['ox', 'x<'],\n",
      " ('xa', 'x<'): ['xa', 'x<'],\n",
      " ('xo', 'x<'): ['xo', 'x<'],\n",
      " ('xx', '<<'): ['xx', '<<', 'ao', 'ax', 'a<', 'oa', 'ox', 'o<', 'xa', 'xo'],\n",
      " ('xx', 'x<'): ['xx', 'x<']}\n"
     ]
    }
   ],
   "source": [
    "def learn_imtsl(data, alphabet, n = 2, context = 2, redacted=False):\n",
    "    \"\"\"\n",
    "    A function that extracts IMTSL grammars. \n",
    "    \n",
    "    Arguments:\n",
    "    data (list): examples from the target language;\n",
    "    alphabet (list or set): symbols of the language;\n",
    "    n (int): size of the target ngrams (available for 2);\n",
    "    context (int): size of the local context considered (available for 2).\n",
    "    \n",
    "    Returns:\n",
    "    dict: keys are tier alphabets, values are tier grammars.\n",
    "    \"\"\"\n",
    "    if n != 2 or context != 2:\n",
    "        raise NotImplementedError(\"This algorithm does not support such values yet.\")\n",
    "        \n",
    "    # collect a list of unattested ngrams (n * context = 2 * 2 = 4)\n",
    "    unattested = unattested_ngrams(data, generate_all_ngrams(alphabet, 4))\n",
    "    \n",
    "    # build a look-up table with the relevant paths\n",
    "    relevant_paths = find_relevant_paths(FL_harmony, unattested)\n",
    "    \n",
    "    # initialize the grammar and the maximum tier\n",
    "    max_tier_guess = generate_all_ngrams(alphabet, context)\n",
    "    grammar = dict()\n",
    "    \n",
    "    for un in unattested:\n",
    "        \n",
    "        local_tier = [un[:2], un[2:]]\n",
    "        local_relevant = [(i[0], set(i[1]), i[2]) for i in relevant_paths[un]]\n",
    "        \n",
    "        for ss in max_tier_guess:\n",
    "            if ss in [un[:2], un[2:]]:\n",
    "                continue\n",
    "                \n",
    "            contain_ss = [i for i in local_relevant if ss in i[1]]\n",
    "            \n",
    "            added = False\n",
    "            for pth in contain_ss:\n",
    "                no_ss_in_path = (pth[0], set(i for i in pth[1] if i != ss), pth[2])\n",
    "                if no_ss_in_path not in local_relevant:\n",
    "                    local_tier.append(ss)\n",
    "                    added = True\n",
    "                    break\n",
    "                if added:\n",
    "                    continue\n",
    "                    \n",
    "        grammar[(un[:2], un[2:])] = local_tier[:]\n",
    "        \n",
    "    if not redacted:\n",
    "        return grammar\n",
    "    \n",
    "    new_grammar = dict()\n",
    "    for pair in grammar:\n",
    "        if tuple(grammar[pair]) not in new_grammar:\n",
    "            new_grammar[tuple(grammar[pair])] = [pair]\n",
    "        else:\n",
    "            new_grammar[tuple(grammar[pair])].append([pair])\n",
    "    \n",
    "    del grammar\n",
    "    return new_grammar\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(learn_imtsl(FL_harmony, sigma1, redacted=False))\n",
    "\n",
    "# for all tier candidates\n",
    "    # check the add condition\n",
    "    # check the remove condition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
